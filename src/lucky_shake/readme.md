
# 5th Solution

## Citeseq

### Data preprocessing and feature engineering

I used two different methods

1.  Preprocessing method of public notebook from
    [@pourchot](https://www.kaggle.com/pourchot).
2.  Using PCA to reduce dimensions to 128 + Direct features based on
    absolute correlation to targets.

I have normalized the row after both methods. The first method is more
effective, and the second method is only used for ensemble.

### Model

1.MLP without BN and drop (Adam as the optimizer). I tried different
activation functions and ensemble them, and this greatly improved CV. 2.
LGBM. I trained two LGBM models (different data preprocessing).

### CV

1.  groupkfold on donor
2.  groupkfold on donor and day

The first one scored higher on public LB, but the second one performed
slightly better on private LB.

### ensemble

Through oof prediction, I selected 4 NN models to mix with 2 LGBM
models, and determined the weight. The best CV score was 0.896(donor).

## Multiome

### Data preprocessing and feature engineering

I used the top method of last year and made some adjustments. This
method has the following steps:

1.  `tf-idf`
2.  `log1p`
3.  `sklearn.preprocessing.Normalizer(norm="l2")` or
    `sklearn.preprocessing.Normalizer(norm="max")`
4.  `PCA(512)`
5.  row normalization
6.  Select the first 64 items in 512 and the first 100 items in 512
    generated by direct dimension reduction as all features.

### Model

MLP with drop(AdamW as the optimizer). I used different activation
functions and ensemble them.

### CV

1.  groupkfold on donor
2.  groupkfold on donor and day

### ensemble

I used 4 NN and ensembled them. The best CV score was 0.670(donor).

In this way, I got the submission of 0.814 public LB and 0.772 private
LB. I think this is probably the easiest way to win the gold medal.
